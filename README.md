#  Pricing Analytics Dashboard

An end-to-end pricing analytics pipeline designed to help business stakeholders understand price sensitivity, discount effectiveness, and salesperson performance â€” with interactive dashboards powered by Streamlit and data hosted on Supabase (PostgreSQL).

---

##  Live App

 [Launch the Dashboard](https://pricinganalyticspipeline.streamlit.app/)

---

##  Project Structure

```
pricing-analytics-pipeline/
â”œâ”€â”€ config/                  # Contains the local .env (ignored by Git)
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ pricing_dashboard/   # Main Streamlit app
â”‚       â”œâ”€â”€ app.py
â”‚       â”œâ”€â”€ pages/
â”‚       â”œâ”€â”€ utils.py
â”‚       â”œâ”€â”€ requirements.txt   
â”‚       â”œâ”€â”€ .streamlit/      # App config + secrets.toml
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ notebooks/               # Experimentation (feature engineering, modeling)
â”‚   â”œâ”€â”€ pricing_simulation.ipynb
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ sql/                     # All core SQL models
â”‚   â”œâ”€â”€ customer_segments.sql
â”‚   â”œâ”€â”€ order_loss_analysis.sql
â”‚   â”œâ”€â”€ ...
â”‚
|                         # Automations
â”œâ”€â”€ main.sh               # Automates ETL 
â””â”€â”€ test.sh
â”œâ”€â”€ db_views.py           # Auto-generates SQL views on startup
â”œâ”€â”€ data/                 # (Optional) Raw CSVs before loading into DB
â”‚
â””â”€â”€ README.md
```

---

##  ETL & Data Modeling

1. **Raw Data Source**: Amazon-style order + sales data, stored locally or in Supabase.
2. **ETL Pipeline**:
   - Built using Python and Bash scripts (`main.sh`)
   - Loads clean CSVs into PostgreSQL tables
   - Testing and validation of data integrity of ETL by (`test.sh`)
3. **SQL Modeling** (views created by `db_views.py`):
   - `order_loss_analysis`: Tracks revenue loss by region, product, payment method
   - `product_pricing_metric`: Derives price bands and sensitivity
   - `customer_segments`: RFM-based customer classification
   - `discount_impact_analysis`: Revenue/profitability of discounts
   - `salesperson_summary`: Individual performance trends
   - `region_behaviour_matrix`: Price elasticity by market
   - `clv_by_segments`: Lifetime value estimation

---

##  Notebooks: Experimentation & Simulation

All advanced logic like clustering, regression models, discount uplift calculations, and â€œwhat-ifâ€ simulations are developed in Jupyter Notebooks. Output tables from these notebooks are saved to Supabase as intermediate views:

- `salesperson_cluster_summary`
- `sensitivity_ranked`
- `discount_simulation_results`
- `what_if_simulation_value_results`

---

##  Dashboard: Streamlit App

The front-end is built using Streamlit with multiple tabs:

1. **Overview**: Executive summary, KPIs, and revenue insights
2. **Price Sensitivity**: Product-region bands, elasticity curves
3. **Discount Effectiveness**: Discount band impact vs revenue/profit
4. **Salespeople Performance**: Growth, clusters, outliers
5. **What-If Simulations**: Forecasting revenue based on discount adjustments

 Data is fetched from Supabase via SQLAlchemy and rendered as charts/tables.

---

##  Secrets & Deployment

- Local development uses a `.env` inside `config/` (not tracked by Git).
- For deployment, secrets are added manually in streamlit for best practice and the secrets.toml is pushed blank.
- App is deployed via [Streamlit Cloud](https://streamlit.io/cloud) and uses Supabase as the backend.

---

##  Automation & Best Practices

- `main.sh` handles setup, loading, and view creation
- `db_views.py` auto-runs all SQL scripts on launch
- GitHub-safe: `.env` and secrets are managed securely

---

##  Tech Stack

- **Languages**: Python, SQL, Bash
- **Tools**: Streamlit, Supabase (PostgreSQL), SQLAlchemy, Pandas, Jupyter, Power BI (optional), dotenv
- **Deployment**: Streamlit Cloud + Supabase

---

##  Lessons Learned

- Streamlit secrets management vs `.env` workflows
- SQL view optimization for dashboards
- Discount modeling via simulation experiments
- Deployment + connection handling on serverless architecture

---

## ğŸ¤ Contributing

Open to collaboration, especially around:
- Improving cluster logic
- Adding new dashboard modules
- Extending to pricing optimization models
- Enhancing ETL pipeline
- Accurate data for modeling
- Any other suggestions or improvements

---
